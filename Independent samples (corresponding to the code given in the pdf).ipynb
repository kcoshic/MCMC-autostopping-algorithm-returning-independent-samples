{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kushcoshic/anaconda/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "import scipy as sp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code returns **effectively independent samples** for a distribution, using a Metropolis-MCMC simulation. There is an inherent correlation within samples in any Markov-chain based simulation, and using effectively independent samples gives a better sense of the distribution with relatively small number of samples; improving on computational efficiency for subsequent analysis.\n",
    "\n",
    "The Metropolis-MCMC simulation keeps running till the Markov-chains achieve approximate convergence, and the required number of independent samples (specified by us) have been drawn. The algorithm is based on the discussion given by Andrew Gelman in his book (Bayesian Data Analysis). We start by running multiple MCMC chains around the posterior (that we want to sample), and check for approximate convergence by running till the autocorrelations fall to a specifiable choice (Potential Scale reduction factor, $R<1.1$). Since we have multiple chains, the overall sample variance should reflect the **between** and **within** chain variance, which is based on the discussion given in the book (Chapter 11)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example considers a multivariate normal distribution, of dimension $k (=2)$. \n",
    "\n",
    "$f(x)$ gives its general functional form:\n",
    "\n",
    "\\begin{equation}\n",
    "\\boxed{f(x) = (2\\pi)^{-\\frac{k}{2}}| \\mathbf{\\Sigma} |^{- \\frac{1}{2}} e^{- \\frac{1}{2}\\ (x-\\mu)\\prime \\ \\mathbf{\\Sigma}^{-1}\\ (x-\\mu )}\\ \\ , \\ \\ \\ k\\equiv dimension}\n",
    "\\end{equation}\n",
    "\n",
    "- $x$ denotes a general k dimensional vector\n",
    "- $|\\Sigma |$ denotes determinant of the Covariance matrix\n",
    "- $\\mu$ denotes the k dimensional, maxima of the distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This array contains the starting points to be used for the Markov chains\n",
    "x_start = np.array([[0.,0.],[-0.9,0.9],[-0.9,-0.9],[0.9,-0.9],[0.9,0.9]]) \n",
    "\n",
    "# Everywhere in the code I refer the term dimension to the coordinates in vector space defined by the vectors x\n",
    "dimension = len(x_start[0])\n",
    "\n",
    "\n",
    "# f(x)\n",
    "# Multivariate Normal distribution\n",
    "# x0,x1 are the 2 gaussian maxima's\n",
    "x0=np.matrix([2.5,2.5])\n",
    "#x1=np.matrix([-2.5,2.5])\n",
    "\n",
    "# cov is the covariance matrix of the bivariate distribution we need to sample\n",
    "cov=np.matrix([[1.,3./5],[3./5,2.]])\n",
    "det=np.linalg.det(cov)\n",
    "inv=np.linalg.inv(cov)\n",
    "\n",
    "k=dimension       # dimension\n",
    "\n",
    "# The distribution that needs to be sampled\n",
    "def f(x):\n",
    "    x=np.ravel(x)\n",
    "    x=np.matrix(x)\n",
    "    return (((2*np.pi)**(-k/2.))*((det)**(-1/2))*np.exp(-(1./2)*(x-x0)*inv*(x-x0).T))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_eff_list = []\n",
    "mcmc_step_list = []\n",
    "n_eff_list_y = []\n",
    "\n",
    "# Defining a function stepper(x) that returns a random vector around x\n",
    "# The stepper parametrizes the jumping distribution for the Metropolis MCMC simulation\n",
    "covv=1.5*cov              # covariance matrix for the stepper function\n",
    "mean=[0.,0.]              # mean vector for the stepper function\n",
    "\n",
    "def stepper(x):\n",
    "    x=np.ravel(x)\n",
    "    dx = np.random.multivariate_normal([0.,0.], covv, 1)\n",
    "    return x+dx\n",
    "\n",
    "\n",
    "# Parameters to be entered by the user\n",
    "n_eff_min = 1000           # minimum number of effective samples required\n",
    "\n",
    "# mcmc_step needs to be a multiple of 4 (because of warm-up and subsequent slicing into 2 arrays)\n",
    "mcmc_step = 1000          # MCMC iterations step-size\n",
    "R_max = 1.1               # maximum allowed value for R, (Potential scale reduction factor)\n",
    "\n",
    "# used to make sure the simulation keeps running till autocorrelation reaches zero, which might not be the case for a small n_eff_min value given by the user.\n",
    "choice = dimension*['']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description of the algorithm:\n",
    "\n",
    "- The function **run_mcmc** executes the mcmc sampling, subsequent autocorrelation computations, confirming approximate convergence, and computing the final set of independent samples.\n",
    "\n",
    "- The while loop keeps running till:\n",
    "        - number of independent samples obtained are atleast equal to n_eff_min\n",
    "        - R < R_max\n",
    "        - The 3rd condition helps avoid the autocorrelation error (when enough MCMC iterations are not done so that the autocorrelation never reaches zero)\n",
    "\n",
    "- For loop [Line 27], generates the mcmc samples for each dimension; and records the data in x_recorded\n",
    "- warming up discards 1st half of the data in x_recorded and stores in x_warmup\n",
    "- Every chain is split into 2 and made seperate chains (in our example the total chains change from 5 to 10). These are the final mcmc samples to be used for extracting independent samples, and are recorded into x_result\n",
    "- All variables like psi_dot_j_bar, psi_dot_dot_bar etc. follow exactly as described in the book.\n",
    "- I have initially assigned each of these as a null array [ ], and append() values for each dimension/coordinate\n",
    "- All functions, such as variance, var_dagger, variogram, autocorrelation etc. have been defined and implemented accordingly, as mentioned in the book\n",
    "- Inside the while loop is a for loop (in the variable d) that computes for each of the coordinates (=dimension) the functions defined below,\n",
    "- For each scalar estimand $\\psi$, we label the simulations as $\\psi_{ij}(i=1,...,n;j=1,...,m)$ and compute **B** and **W**, the Between- and Within -sequence variances:\n",
    "\n",
    "\\begin{equation}\n",
    "B=\\frac{n}{m-1} \\sum_{j=1}^{m}(\\bar{\\psi_{.j}} - \\bar{\\psi_{..}})^2 \\ , \\ \\ \\ where \\ \\ \\bar{\\psi_{.j}} = \\frac{1}{n}\\sum_{i=1}^{n} \\psi_{ij} \\ , \\ \\ \\ \\ \\bar{\\psi_{..}}=\\frac{1}{m}\\sum_{j=1}^{m}\\bar{\\psi_{.j}}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "W = \\frac{1}{m}\\sum_{j=1}^{m} s_{j}^2 , \\ \\ where \\ s_{j}^2 = \\frac{1}{n-1}\\sum_{i=1}^{n} (\\psi_{ij} - \\bar{\\psi_{.j}})^2\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{var}^{+} (\\psi | y)=\\frac{n-1}{n} W + \\frac{1}{n} B\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{R} = \\sqrt{\\frac{\\hat{var}^{+} (\\psi | y)}{W}}\n",
    "\\end{equation}\n",
    "\n",
    "- An estimate of the effective sample size is obtaines from the asymptotic formula for the variance of the average of a correlated sequemce:\n",
    "\n",
    "\\begin{equation}\n",
    "lim_{n \\rightarrow \\infty} mn \\ var(\\bar{\\psi_{..}}) = \\big( 1+2\\sum_{t=1}^{\\infty} \\rho_{t} \\big) var(\\psi |y)\n",
    "\\end{equation}\n",
    "\n",
    "where $\\rho_{t}$ is the autocorrelation of the sequence $\\psi$ at lag t.\n",
    "\n",
    "- If the n simulation draws from each of the m chains were independent, then $var(\\bar{\\psi_{..}})$ would simply be $\\frac{1}{mn} var(\\psi |y)$ and the sample size would be mn. However in the presence of correlation we can define the **effective sample size** as:\n",
    "\n",
    "\\begin{equation}\n",
    "n_{eff} = \\frac{mn}{1+2\\sum_{t=1}^{\\infty} \\rho_{t}}\n",
    "\\end{equation}\n",
    "\n",
    "- Unfortunately, simply summing all of the autocorrelations to estimate $n_{eff}$ will have the sample correlation too noisy for large values of t. Therefore we compute a partial sum, starting from lag0 and continuing until the sum of autocorrelation estimates for 2 successive lags $\\hat{\\rho}_{2t'}+\\hat{\\rho}_{2t'+1}$ is negative; where\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{n}_{eff} = \\frac{mn}{1+2\\sum_{t=1}^{T} \\hat{\\rho_{t}}}\n",
    "\\end{equation}\n",
    "\n",
    "where T is the first odd positive integer for which $\\hat{rho}_{T+1} + \\hat{rho}_{T+2}$ is negative\n",
    "\n",
    "-Now to get the set of effectively independent samples, we draw samples from x_result, but skipping every $\\frac{mn}{n_{eff}}$ samples in between. Finally we record the final set of independent samples in **samples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kushcoshic/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:138: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/Users/kushcoshic/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:160: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "def run_mcmc(f,x_start,stepper,n_eff_min,R_max,mcmc_step):\n",
    "    n_effective = np.zeros(dimension)\n",
    "    n_eff_min_list = np.zeros(dimension)\n",
    "    acceptance_list=[]\n",
    "    for i in range(dimension):\n",
    "        n_eff_min_list[i] = n_eff_min\n",
    "    R_max_list = np.zeros(dimension)\n",
    "    for i in range(dimension):\n",
    "        R_max_list[i] = R_max\n",
    "    \n",
    "    z=0\n",
    "    R=0\n",
    "    mcmc_step_remember = mcmc_step\n",
    "    x_recorded = np.zeros((mcmc_step,dimension,len(x_start)))\n",
    "\n",
    "    while (np.any(np.array(n_effective)<n_eff_min_list) or np.any(R>R_max_list) and all([i=='' for i in choice])):\n",
    "        # MCMC results stored in a 3-d array, 3rd dimension corresponding to every new chain\n",
    "        #choice = dimension*['']\n",
    "        if z==0:                         # this if else is there to make sure the samples from the previous run are not wasted\n",
    "            mcmc_step_0=0\n",
    "            x_recorded = np.zeros((mcmc_step,dimension,len(x_start)))\n",
    "        else:\n",
    "            x_recorded_template = np.zeros((mcmc_step,dimension,len(x_start)))         # this if-else is used to store the already computed MCMC samples, before moving to further iterations (if n_eff condition not satisfied). This prevents unecessary wastage of computational power.\n",
    "            x_recorded_template[:mcmc_step_0,:,:]=x_recorded[:,:,:]\n",
    "            x_recorded = x_recorded_template\n",
    "        for i in range(len(x_start)):\n",
    "            x_current = x_start[i]\n",
    "            acceptances = np.zeros(mcmc_step)\n",
    "            for k in range(mcmc_step_0,mcmc_step):\n",
    "                x_new = stepper(x_current)\n",
    "                if np.random.uniform(0,1) < f(x_new)/f(x_current):\n",
    "                    x_current = x_new\n",
    "                    acceptances[k] = 1\n",
    "                x_recorded[k,:,i] = x_current\n",
    "            acceptance_list.append(1.0*acceptances.sum()/len(acceptances))\n",
    "            #print 'acceptance fraction = %f'%(1.0*acceptances.sum()/len(acceptances))\n",
    "\n",
    "        # Warm up period (extracting only second half of the iterations)\n",
    "        x_warmup = np.zeros((mcmc_step/2,dimension,len(x_start)))\n",
    "        x_warmup[:,:,:] = x_recorded[mcmc_step/2:,:,:]\n",
    "        #plt.plot(x_warmup[:,0,:],x_warmup[:,1,:],'.',color='b')\n",
    "\n",
    "        # Splitting the chains\n",
    "        x_result = np.zeros((mcmc_step/4,dimension,2*len(x_start)))\n",
    "        x_result[:,:,0:len(x_start)] = x_warmup[0:mcmc_step/4,:,:]\n",
    "        x_result[:,:,len(x_start):2*len(x_start)] = x_warmup[mcmc_step/4:mcmc_step/2,:,:]\n",
    "        \n",
    "        \n",
    "        n=mcmc_step/4.0         # number of iterations in each chain\n",
    "        m=2.0*(len(x_start))    # number of chains\n",
    "\n",
    "        \n",
    "        # Assessing mixing using between and within sequence variances\n",
    "\n",
    "        psi = []\n",
    "        for i in range(dimension):\n",
    "            psi.append(x_result[:,i,:])\n",
    "        psi = np.array(psi)\n",
    "        # this was done to divide the data for each dimension, for convenience\n",
    "        \n",
    "        psi_dot_j_bar = []\n",
    "        psi_dot_dot_bar = []\n",
    "        B_term = []\n",
    "        B = []                                    # Between sequence variance\n",
    "        s_j_square_term = []\n",
    "        s_j_square = []\n",
    "        W = []                                    # Within sequence variance\n",
    "        var_dagger = []\n",
    "        R = []\n",
    "        psi_i_comma_j = []\n",
    "        psi_i_minus_t_comma_j = []\n",
    "        \n",
    "        rho_array_set = []\n",
    "        t_array_set = []\n",
    "        n_effective_list = []\n",
    "        t_array = np.arange(int(n))\n",
    "        \n",
    "        \n",
    "        # for every subsequent dimension the values are appended into the arrays\n",
    "        for d in range(dimension):\n",
    "            psi_dot_j_bar.append((1/n)*np.sum(psi[d], axis=0))\n",
    "            \n",
    "            psi_dot_dot_bar.append((1/m)*np.sum(psi_dot_j_bar[d]))\n",
    "            \n",
    "            B_term.append(psi_dot_j_bar[d] - psi_dot_dot_bar[d])\n",
    "            \n",
    "            B.append((n/(m-1.0))*np.sum(B_term[d]**2))\n",
    "\n",
    "            s_j_square_term.append(np.zeros((int(n),int(m))))\n",
    "            \n",
    "            for j in range(int(m)):         # performing for all chains\n",
    "                s_j_square_term[d][:,j] = (psi[d][:,j] - psi_dot_j_bar[d][j])**2.0\n",
    "\n",
    "            s_j_square.append((1.0/(n-1))*np.sum(s_j_square_term[d], axis=0))\n",
    "            \n",
    "            W.append((1.0/(m))*np.sum(s_j_square[d]))\n",
    "            \n",
    "\n",
    "            # variance\n",
    "            var_dagger.append(((n-1)/(n))*W[d] + (1/n)*B[d])\n",
    "            \n",
    "            #var_dagger\n",
    "\n",
    "            # potential scale reduction\n",
    "            R.append(np.sqrt(var_dagger[d]/W[d]))\n",
    "            \n",
    "            #R\n",
    "            # Variogram\n",
    "            def V(t):\n",
    "                psi_i_comma_j = psi[d][t+1-1:,:]\n",
    "                \n",
    "                psi_i_minus_t_comma_j = psi[d][0:int(n)-t,:]\n",
    "                \n",
    "                t=float(t)\n",
    "                return (1.0/(m*(n-t)))*np.sum(np.sum((psi_i_comma_j - psi_i_minus_t_comma_j)**2.0,axis=0))\n",
    "            \n",
    "            # autocorrelation\n",
    "            def rho(t):\n",
    "                return 1.0-(V(t)/(2.0*var_dagger[d]))\n",
    "            \n",
    "            \n",
    "            rho_array=np.zeros(int(n))\n",
    "            for t in range(int(n)):\n",
    "                rho_array[t]= rho(int(t))\n",
    "            rho_array_set.append(rho_array)\n",
    "            # checking where the autocorrelation goes to zero for the first time, append that iteration number in choice array for every subsequent dimension\n",
    "            # The while loop in the beginning, makes the code run until all k (=dimension) elements in choice[] are not null, which implies autocorrelation reached zero for each of them.\n",
    "            for i in range(int(n)):\n",
    "                check = rho(i) + rho(i+1)\n",
    "                if check < 0:\n",
    "                    #choice.append(i*1.0)\n",
    "                    choice[d]=i*1.0\n",
    "                    break\n",
    "            \n",
    "            # if choice[d] is not null, autocorrelation did reach zero and we could use the corresponding iteration number to calculate the value for n_effective obtained for the simulation\n",
    "            if choice[d] != '':\n",
    "                # effective sample size\n",
    "                n_effective[d]=(m*n)/(1+2.0*np.sum(rho_array[1:choice[d]]))\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        mcmc_step_0 = mcmc_step                     # record current number of mcmc runs in mcmc_step_0, for the next run\n",
    "        mcmc_step += mcmc_step_remember             # increase number of mcmc runs to be performed next in the next while loop run\n",
    "        z += 1.0                                    # to keep track of the number of while loop runs (=z after run_mcmc ends)\n",
    "        \n",
    "    \n",
    "    # Taking the minimum value for n_effective \n",
    "    n_effective_final = np.amin(n_effective)\n",
    "    \n",
    "    # no. of samples to be skipped before recording the next independent sample\n",
    "    samples_skip = 1.0*x_result.shape[2]*x_result.shape[0]/n_effective_final\n",
    "    samples_skip=np.ceil(samples_skip)\n",
    "    \n",
    "    # extract and store independent samples in samples_final \n",
    "    if int(x_result.shape[0]/samples_skip)==x_result.shape[0]/samples_skip:\n",
    "        samples_final = np.zeros((int((x_result.shape[0])/samples_skip),x_result.shape[1],x_result.shape[2]))\n",
    "    else:\n",
    "        samples_final = np.zeros((int(np.ceil((x_result.shape[0])/samples_skip)),x_result.shape[1],x_result.shape[2]))\n",
    "    # Broadcasting independent samples into samples_final\n",
    "    samples_final[:,:,:] = x_result[0::samples_skip,:,:]\n",
    "    # note that at this point the simulation data from each chain is recorded separately, it will be condensed in samples,\n",
    "    \n",
    "    # putting all points of different chains into 1 single array which can be referred to as the final array of our independent samples\n",
    "    samples = np.zeros((samples_final.shape[0]*samples_final.shape[2],dimension))\n",
    "    for i in range(samples_final.shape[2]):\n",
    "        samples[i*samples_final.shape[0]:(i+1)*samples_final.shape[0],:]=samples_final[:,:,i]\n",
    "    \n",
    "    return n_effective,R,z,x_recorded.shape,x_result.shape,x_result,samples_final,samples,acceptance_list,x_recorded,choice\n",
    "\n",
    "\n",
    "run = run_mcmc(f,x_start,stepper,n_eff_min,R_max,mcmc_step)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_final=run[6]\n",
    "\n",
    "samples_final_mean = np.zeros(dimension)\n",
    "samples_final_mean_variance = np.zeros(dimension)\n",
    "for i in range(len(samples_final_mean)):\n",
    "    samples_final_mean[i] = np.mean(samples_final[:,i,:])                           # sample mean\n",
    "    samples_final_mean_variance[i] = np.var(samples_final[:,i,:])                   # sample variance\n",
    "    \n",
    "#print samples_final_mean,samples_final_mean_variance\n",
    "variance = (samples_final_mean_variance[0]**2.0 + samples_final_mean_variance[1]**2.0)**(1/2.0)\n",
    "#print variance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # Autocorrelation plots in the final set of effectively independent samples\n",
    "# As we should expect, the autocorrelation plot should stay around the zero value (whichi it does perfectly if mcmc_step is taken large)\n",
    "\n",
    "# Define for convenience\n",
    "n = samples_final.shape[0]*1.0                   # no. of iterations for every chain\n",
    "m = samples_final.shape[2]*1.0                   # no. of final chains\n",
    "sample_x = samples_final[:,0,:]                  # samples x-dimension data\n",
    "sample_y = samples_final[:,1,:]                  # samples y-dimension data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgEAAAFHCAYAAAA4D5+NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10b152510>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def V(t):                                        # same as defined inside run_mcmc\n",
    "    sample_x_i_comma_j = sample_x[t+1-1:,:]\n",
    "    sample_x_i_minus_t_comma_j = sample_x[0:int(n)-t,:]\n",
    "    t=float(t)\n",
    "    return (1/(m*(n-t)))*np.sum(np.sum((sample_x_i_comma_j - sample_x_i_minus_t_comma_j)**2.0,axis=0))\n",
    "\n",
    "def rho(t):                                      # same as defined inside run_mcmc\n",
    "    return 1-(V(t)/(2.0*samples_final_mean_variance[0]))\n",
    "\n",
    "rho_array = np.zeros(int(n))\n",
    "t_array = np.arange(int(n))\n",
    "\n",
    "for t in range(int(n)):\n",
    "    rho_array[t]= rho(int(t))                   # array containing rho values for the x coordinate\n",
    "    \n",
    "\n",
    "# Doing the same for the y-coordinate\n",
    "def V_y(t):                                     # the same as above, but for the y coordinate\n",
    "    sample_y_i_comma_j = sample_y[t+1-1:,:]\n",
    "    sample_y_i_minus_t_comma_j = sample_y[0:int(n)-t,:]\n",
    "    t=float(t)\n",
    "    return (1/(m*(n-t)))*np.sum(np.sum((sample_y_i_comma_j - sample_y_i_minus_t_comma_j)**2.0,axis=0))\n",
    "\n",
    "def rho_y(t):\n",
    "    return 1-(V_y(t)/(2.0*samples_final_mean_variance[1]))\n",
    "\n",
    "rho_array_y = np.zeros(int(n))\n",
    "t_array = np.arange(int(n))\n",
    "\n",
    "for t in range(int(n)):\n",
    "    rho_array_y[t]= rho_y(int(t))              # array containing rho values for the y coordinate\n",
    "    \n",
    "    \n",
    "plt.figure(2,figsize=(8,5))\n",
    "plt.plot(t_array,rho_array,'-',color='b')                    # autocorrelation plot for x coordinate\n",
    "plt.plot(t_array,rho_array_y,'-',color='r')                  # autocorrelation plot for y coordinate\n",
    "plt.xlabel('lag')\n",
    "plt.ylabel('autocorrelation')\n",
    "plt.show()\n",
    "\n",
    "# The 2 curves seem correlated which it should since f(x) has the 2 coordinates correlated. \n",
    "# If f(x) had the 2 coordinates independent then this sort of curve could have meant some issue with the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking how close the covariance matrix obtained from the independent samples is to what's defined in f(x)\n",
    "np.allclose(np.cov(run[7].T),cov,.06,.06)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.05984907,  0.6769533 ],\n",
       "       [ 0.6769533 ,  2.04146005]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# covariance matrix calculated from the samples\n",
    "np.cov(run[7].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
