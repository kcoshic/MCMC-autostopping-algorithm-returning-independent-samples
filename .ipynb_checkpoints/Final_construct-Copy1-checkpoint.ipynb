{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kushcoshic/anaconda/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "import scipy as sp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code returns **effectively independent samples** for a distribution, using a Metropolis-MCMC simulation. There is an inherent correlation within samples in any Markov-chain based simulation, and using effectively independent samples gives a better sense of the distribution with relatively small number of samples; improving on computational efficiency for subsequent analysis.\n",
    "\n",
    "The Metropolis-MCMC simulation keeps running till the Markov-chains achieve approximate convergence, and the required number of independent samples (specified by us) have been drawn. The algorithm is based on the discussion given by Andrew Gelman in his book (Bayesian Data Analysis). We start by running multiple MCMC chains around the posterior (that we want to sample), and check for approximate convergence by running till the autocorrelations fall to a specifiable choice (Potential Scale reduction factor, $R<1.1$). Since we have multiple chains, the overall sample variance should reflect the **between** and **within** chain variance, which is based on the discussion given in the book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example considers a multivariate normal distribution, of dimension $k (=2)$. \n",
    "\n",
    "$f(x)$ gives its general functional form:\n",
    "\n",
    "\\begin{equation}\n",
    "\\boxed{f(x) = (2\\pi)^{-\\frac{k}{2}}| \\mathbf{\\Sigma} |^{- \\frac{1}{2}} e^{- \\frac{1}{2}\\ (x-\\mu)\\prime \\ \\mathbf{\\Sigma}^{-1}\\ (x-\\mu )}\\ \\ , \\ \\ \\ k\\equiv dimension}\n",
    "\\end{equation}\n",
    "\n",
    "- $x$ denotes a general k dimensional vector\n",
    "- $|\\Sigma |$ denotes determinant of the Covariance matrix\n",
    "- $\\mu$ denotes the k dimensional, maxima of the distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_start = np.array([[0.,0.],[-0.9,0.9],[-0.9,-0.9],[0.9,-0.9],[0.9,0.9]]) \n",
    "\n",
    "# Everywhere in the code I refer the term dimension to a coordinate in the vector space defined by the vectos x\n",
    "dimension = len(x_start[0])\n",
    "\n",
    "# Multivariate Normal distribution\n",
    "\n",
    "# x0,x1 are the 2 gaussian maxima's\n",
    "x0=np.matrix([2.5,2.5])\n",
    "x1=np.matrix([-2.5,2.5])\n",
    "\n",
    "# cov is the covariance matrix of the bivariate distribution we need to sample\n",
    "cov=np.matrix([[1.,3./5],[3./5,2.]])\n",
    "det=np.linalg.det(cov)\n",
    "inv=np.linalg.inv(cov)\n",
    "\n",
    "k=dimension       # dimension\n",
    "\n",
    "# The distribution that needs to be sampled\n",
    "def f(x):\n",
    "    x=np.ravel(x)\n",
    "    x=np.matrix(x)\n",
    "    return (((2*np.pi)**(-k/2.))*((det)**(-1/2))*np.exp(-(1./2)*(x-x0)*inv*(x-x0).T))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_eff_list = []\n",
    "mcmc_step_list = []\n",
    "n_eff_list_y = []\n",
    "\n",
    "# Defining a function stepper(x) that returns a random vector around x\n",
    "# The stepper parametrizes the jumping distribution for the Metropolis MCMC simulation\n",
    "covv=1.5*cov              # covariance matrix for the stepper function\n",
    "mean=[0.,0.]              # mean vector for the stepper function\n",
    "\n",
    "def stepper(x):\n",
    "    x=np.ravel(x)\n",
    "    dx = np.random.multivariate_normal([0.,0.], covv, 1)\n",
    "    return x+dx\n",
    "\n",
    "\n",
    "# Parameters to be entered by the user\n",
    "n_eff_min = 120           # minimum number of effective samples required\n",
    "\n",
    "# mcmc_step needs to be a multiple of 4 (because of warm-up and subsequent slicing into 2 arrays)\n",
    "mcmc_step = 400          # MCMC iterations step-size\n",
    "R_max = 1.1               # maximum allowed value for R, (Potential scale reduction factor)\n",
    "\n",
    "# used to make sure the simulation keeps running till autocorrelation reaches zero, which might not be the case for a small n_eff_min value given by the user.\n",
    "choice = ['']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description of the algorithm:\n",
    "\n",
    "- The function **run_mcmc** executes the mcmc sampling, subsequent autocorrelation computations, confirming approximate convergence, and computing the final set of independent samples.\n",
    "\n",
    "- The while loop keeps running till:\n",
    "        - number of independent samples obtained are atleast equal to n_eff_min\n",
    "        - R < R_max\n",
    "        - The 3rd condition helps avoid the autocorrelation error (when enough MCMC iterations are not done so that the autocorrelation never reaches zero)\n",
    "\n",
    "- For loop [Line 27], generates the mcmc samples for each dimension; and records the data in x_recorded\n",
    "- warming up discards 1st half of the data in x_recorded and stores in x_warmup\n",
    "- Every chain is split into 2 and made seperate chains (in our example the total chains change from 5 to 10). These are the final mcmc samples to be used for extracting independent samples, and are recorded into x_result\n",
    "- All variables like psi_dot_j_bar, psi_dot_dot_bar etc. follow exactly as described in the book.\n",
    "- I have initially assigned each of these as a null array [ ], and append() values for each dimension/coordinate\n",
    "- All functions, such as variance, var_dagger, variogram, autocorrelation etc. have been defined and implemented accordingly, as mentioned in the book\n",
    "- \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kushcoshic/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:138: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list assignment index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-4c9e4259e134>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m \u001b[0mrun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_mcmc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_start\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstepper\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_eff_min\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mR_max\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmcmc_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-4c9e4259e134>\u001b[0m in \u001b[0;36mrun_mcmc\u001b[0;34m(f, x_start, stepper, n_eff_min, R_max, mcmc_step)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0mcheck\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrho\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrho\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcheck\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                     \u001b[0mchoice\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list assignment index out of range"
     ]
    }
   ],
   "source": [
    "\n",
    "def run_mcmc(f,x_start,stepper,n_eff_min,R_max,mcmc_step):\n",
    "    n_effective = np.zeros(dimension)\n",
    "    n_eff_min_list = np.zeros(dimension)\n",
    "    acceptance_list=[]\n",
    "    for i in range(dimension):\n",
    "        n_eff_min_list[i] = n_eff_min\n",
    "    R_max_list = np.zeros(dimension)\n",
    "    for i in range(dimension):\n",
    "        R_max_list[i] = R_max\n",
    "    \n",
    "    z=0\n",
    "    R=0\n",
    "    mcmc_step_remember = mcmc_step\n",
    "    x_recorded = np.zeros((mcmc_step,dimension,len(x_start)))\n",
    "\n",
    "    while (np.any(np.array(n_effective)<n_eff_min_list) or np.any(R>R_max_list) and any([i=='' for i in choice[1:]])):\n",
    "        # MCMC results stored in a 3-d array, 3rd dimension corresponding to every new chain\n",
    "        #choice = dimension*['']\n",
    "        if z==0:                         # this if else is there to make sure the samples from the previous run are not wasted\n",
    "            mcmc_step_0=0\n",
    "            x_recorded = np.zeros((mcmc_step,dimension,len(x_start)))\n",
    "        else:\n",
    "            x_recorded_template = np.zeros((mcmc_step,dimension,len(x_start)))         # this if-else is used to store the already computed MCMC samples, before moving to further iterations (if n_eff condition not satisfied). This prevents unecessary wastage of computational power.\n",
    "            x_recorded_template[:mcmc_step_0,:,:]=x_recorded[:,:,:]\n",
    "            x_recorded = x_recorded_template\n",
    "        for i in range(len(x_start)):\n",
    "            x_current = x_start[i]\n",
    "            acceptances = np.zeros(mcmc_step)\n",
    "            for k in range(mcmc_step_0,mcmc_step):\n",
    "                x_new = stepper(x_current)\n",
    "                if np.random.uniform(0,1) < f(x_new)/f(x_current):\n",
    "                    x_current = x_new\n",
    "                    acceptances[k] = 1\n",
    "                x_recorded[k,:,i] = x_current\n",
    "            acceptance_list.append(1.0*acceptances.sum()/len(acceptances))\n",
    "            #print 'acceptance fraction = %f'%(1.0*acceptances.sum()/len(acceptances))\n",
    "\n",
    "        # Warm up period (extracting only second half of the iterations)\n",
    "        x_warmup = np.zeros((mcmc_step/2,dimension,len(x_start)))\n",
    "        x_warmup[:,:,:] = x_recorded[mcmc_step/2:,:,:]\n",
    "        #plt.plot(x_warmup[:,0,:],x_warmup[:,1,:],'.',color='b')\n",
    "\n",
    "        # Splitting the chains\n",
    "        x_result = np.zeros((mcmc_step/4,dimension,2*len(x_start)))\n",
    "        x_result[:,:,0:len(x_start)] = x_warmup[0:mcmc_step/4,:,:]\n",
    "        x_result[:,:,len(x_start):2*len(x_start)] = x_warmup[mcmc_step/4:mcmc_step/2,:,:]\n",
    "        \n",
    "        \n",
    "        n=mcmc_step/4.0         # number of iterations in each chain\n",
    "        m=2.0*(len(x_start))    # number of chains\n",
    "\n",
    "        \n",
    "        # Assessing mixing using between and within sequence variances\n",
    "\n",
    "        psi = []\n",
    "        for i in range(dimension):\n",
    "            psi.append(x_result[:,i,:])\n",
    "        psi = np.array(psi)\n",
    "        # this was done to divide the data for each dimension, for convenience\n",
    "        \n",
    "        psi_dot_j_bar = []\n",
    "        psi_dot_dot_bar = []\n",
    "        B_term = []\n",
    "        B = []                                    # Between sequence variance\n",
    "        s_j_square_term = []\n",
    "        s_j_square = []\n",
    "        W = []                                    # Within sequence variance\n",
    "        var_dagger = []\n",
    "        R = []\n",
    "        psi_i_comma_j = []\n",
    "        psi_i_minus_t_comma_j = []\n",
    "        \n",
    "        rho_array_set = []\n",
    "        t_array_set = []\n",
    "        n_effective_list = []\n",
    "        t_array = np.arange(int(n))\n",
    "        \n",
    "        \n",
    "        # for every subsequent dimension the values are appended into the arrays\n",
    "        for d in range(dimension):\n",
    "            psi_dot_j_bar.append((1/n)*np.sum(psi[d], axis=0))\n",
    "            \n",
    "            psi_dot_dot_bar.append((1/m)*np.sum(psi_dot_j_bar[d]))\n",
    "            \n",
    "            B_term.append(psi_dot_j_bar[d] - psi_dot_dot_bar[d])\n",
    "            \n",
    "            B.append((n/(m-1.0))*np.sum(B_term[d]**2))\n",
    "\n",
    "            s_j_square_term.append(np.zeros((int(n),int(m))))\n",
    "            \n",
    "            for j in range(int(m)):         # performing for all chains\n",
    "                s_j_square_term[d][:,j] = (psi[d][:,j] - psi_dot_j_bar[d][j])**2.0\n",
    "\n",
    "            s_j_square.append((1.0/(n-1))*np.sum(s_j_square_term[d], axis=0))\n",
    "            \n",
    "            W.append((1.0/(m))*np.sum(s_j_square[d]))\n",
    "            \n",
    "\n",
    "            # variance\n",
    "            var_dagger.append(((n-1)/(n))*W[d] + (1/n)*B[d])\n",
    "            \n",
    "            #var_dagger\n",
    "\n",
    "            # potential scale reduction\n",
    "            R.append(np.sqrt(var_dagger[d]/W[d]))\n",
    "            \n",
    "            #R\n",
    "            # Variogram\n",
    "            def V(t):\n",
    "                psi_i_comma_j = psi[d][t+1-1:,:]\n",
    "                \n",
    "                psi_i_minus_t_comma_j = psi[d][0:int(n)-t,:]\n",
    "                \n",
    "                t=float(t)\n",
    "                return (1.0/(m*(n-t)))*np.sum(np.sum((psi_i_comma_j - psi_i_minus_t_comma_j)**2.0,axis=0))\n",
    "            \n",
    "            # autocorrelation\n",
    "            def rho(t):\n",
    "                return 1.0-(V(t)/(2.0*var_dagger[d]))\n",
    "            \n",
    "            \n",
    "            rho_array=np.zeros(int(n))\n",
    "            for t in range(int(n)):\n",
    "                rho_array[t]= rho(int(t))\n",
    "            rho_array_set.append(rho_array)\n",
    "            # checking where the autocorrelation goes to zero for the first time, append that iteration number in choice array for every subsequent dimension\n",
    "            # The while loop in the beginning, makes the code run until all k (=dimension) elements in choice[] are not null, which implies autocorrelation reached zero for each of them.\n",
    "            for i in range(int(n)):\n",
    "                check = rho(i) + rho(i+1)\n",
    "                if check < 0:\n",
    "                    choice[d]=i*1.0\n",
    "                    break\n",
    "            \n",
    "            # if choice[d] is not null, autocorrelation did reach zero and we could use the corresponding iteration number to calculate the value for n_effective obtained for the simulation\n",
    "            if choice[d] != '':\n",
    "                # effective sample size\n",
    "                n_effective[d]=(m*n)/(1+2.0*np.sum(rho_array[1:choice[d]]))\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        mcmc_step_0 = mcmc_step                     # record current number of mcmc runs in mcmc_step_0, for the next run\n",
    "        mcmc_step += mcmc_step_remember             # increase number of mcmc runs to be performed next in the next while loop run\n",
    "        z += 1.0                                    # to keep track of the number of while loop runs (=z-1)\n",
    "        \n",
    "    \n",
    "    # Taking the minimum value for n_effective \n",
    "    n_effective_final = np.amin(n_effective)\n",
    "    \n",
    "    # no. of samples to be skipped before recording the next independent sample\n",
    "    samples_skip = 1.0*x_result.shape[2]*x_result.shape[0]/n_effective_final\n",
    "    samples_skip=np.ceil(samples_skip)\n",
    "    \n",
    "    # extract and store independent samples in samples_final \n",
    "    if int(x_result.shape[0]/samples_skip)==x_result.shape[0]/samples_skip:\n",
    "        samples_final = np.zeros((int((x_result.shape[0])/samples_skip),x_result.shape[1],x_result.shape[2]))\n",
    "    else:\n",
    "        samples_final = np.zeros((int(np.ceil((x_result.shape[0])/samples_skip)),x_result.shape[1],x_result.shape[2]))\n",
    "    # Broadcasting independent samples into samples_final\n",
    "    samples_final[:,:,:] = x_result[0::samples_skip,:,:]\n",
    "    # note that at this point the simulation data from each chain is recorded separately, it will be condensed in samples,\n",
    "    \n",
    "    # putting all points of different chains into 1 single array which can be referred to as the final array of our independent samples\n",
    "    samples = np.zeros((samples_final.shape[0]*samples_final.shape[2],dimension))\n",
    "    for i in range(samples_final.shape[2]):\n",
    "        samples[i*samples_final.shape[0]:(i+1)*samples_final.shape[0],:]=samples_final[:,:,i]\n",
    "    \n",
    "    return n_effective,R,z-1,x_recorded.shape,x_result.shape,x_result,samples_final,samples,acceptance_list,x_recorded\n",
    "\n",
    "\n",
    "run = run_mcmc(f,x_start,stepper,n_eff_min,R_max,mcmc_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "samples_final=run[6]\n",
    "\n",
    "samples_final_mean = np.zeros(dimension)\n",
    "samples_final_mean_variance = np.zeros(dimension)\n",
    "for i in range(len(samples_final_mean)):\n",
    "    samples_final_mean[i] = np.mean(samples_final[:,i,:])                           # sample mean\n",
    "    samples_final_mean_variance[i] = np.var(samples_final[:,i,:])                   # sample variance\n",
    "    \n",
    "print samples_final_mean,samples_final_mean_variance\n",
    "variance = (samples_final_mean_variance[0]**2.0 + samples_final_mean_variance[1]**2.0)**(1/2.0)\n",
    "print variance\n",
    "\n",
    "# Top view plot for effectively independent samples\n",
    "plt.plot(samples_final[:,0,:],samples_final[:,1,:],'.',color='blue') \n",
    "plt.plot(samples_final_mean[0],samples_final_mean[1],'o',color='red')\n",
    "plt.plot(x0[0,0],x0[0,1],'o',color='g')\n",
    "\n",
    "print \"difference between actual mean point and mean point from the samples = %f\" % (np.linalg.norm(x0 - samples_final_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # Autocorrelation plots in the final set of effectively independent samples\n",
    "\n",
    "n = samples_final.shape[0]*1.0                   # no. of iterations for every chain\n",
    "m = samples_final.shape[2]*1.0                   # no. of final chains\n",
    "\n",
    "sample_x = samples_final[:,0,:]                  # samples x-dimension data\n",
    "sample_y = samples_final[:,1,:]                  # samples y-dimension data\n",
    "\n",
    "def V(t):                                        # same as defined inside run_mcmc\n",
    "    sample_x_i_comma_j = sample_x[t+1-1:,:]\n",
    "    sample_x_i_minus_t_comma_j = sample_x[0:int(n)-t,:]\n",
    "    t=float(t)\n",
    "    return (1/(m*(n-t)))*np.sum(np.sum((sample_x_i_comma_j - sample_x_i_minus_t_comma_j)**2.0,axis=0))\n",
    "\n",
    "def rho(t):                                      # same as defined inside run_mcmc\n",
    "    return 1-(V(t)/(2.0*samples_final_mean_variance[0]))\n",
    "\n",
    "rho_array = np.zeros(int(n))\n",
    "t_array = np.arange(int(n))\n",
    "\n",
    "for t in range(int(n)):\n",
    "    rho_array[t]= rho(int(t))                   \n",
    "    \n",
    "\n",
    "# Doing the same for the y-coordinate\n",
    "def V_y(t):                                     # the same as above, but for the y coordinate\n",
    "    sample_y_i_comma_j = sample_y[t+1-1:,:]\n",
    "    sample_y_i_minus_t_comma_j = sample_y[0:int(n)-t,:]\n",
    "    t=float(t)\n",
    "    return (1/(m*(n-t)))*np.sum(np.sum((sample_y_i_comma_j - sample_y_i_minus_t_comma_j)**2.0,axis=0))\n",
    "\n",
    "def rho_y(t):\n",
    "    return 1-(V_y(t)/(2.0*samples_final_mean_variance[1]))\n",
    "\n",
    "rho_array_y = np.zeros(int(n))\n",
    "t_array = np.arange(int(n))\n",
    "\n",
    "for t in range(int(n)):\n",
    "    rho_array_y[t]= rho_y(int(t))\n",
    "    \n",
    "    \n",
    "plt.figure(2,figsize=(8,5))\n",
    "plt.plot(t_array,rho_array,'-',color='b')                    # autocorrelation plot for x coordinate\n",
    "plt.plot(t_array,rho_array_y,'-',color='r')                  # autocorrelation plot for y coordinate\n",
    "plt.show()\n",
    "\n",
    "# The 2 curves seem correlated which it should since f(x) has the 2 coordinates correlated. \n",
    "# If f(x) had the 2 coordinates independent then this sort of curve could have meant some issue with the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mu = np.mean(run[7],axis=0)            # variance in samples\n",
    "var = np.var(run[7],axis=0)            # variance in samples\n",
    "N = run[7].shape[0]                    # no. of samples\n",
    "z = (mu-0)/np.sqrt(var/N)              # z-value of the samples\n",
    "print z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(1,figsize=(5,5))\n",
    "plt.plot(run[5][:,0,:],run[5][:,1,:],'.',color='b')                 # x_result (i.e. data after warmup and splicing)\n",
    "plt.plot(x_start[:,0],x_start[:,1],'or')                            # starting points of the Markov chains used\n",
    "plt.plot(run[7][:,0],run[7][:,1],'.',color='orange')                # final samples\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# final samples returned\n",
    "run[7].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Checking how close the covariance matrix obtained from the independent samples is to what's defined in f(x)\n",
    "np.allclose(np.cov(run[7].T),cov,.06,.06)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.cov(run[7].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data=[i=='' for i in ['k','','']]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(choice[i]=='' for i in range(dimension))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
